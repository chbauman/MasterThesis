\documentclass[a4paper]{article}

\usepackage[citestyle=alphabetic,style=alphabetic, backend=bibtex]{biblatex}

% Make it compact
\usepackage[compact]{titlesec}
\usepackage{geometry}
\geometry{scale=.8}% use 80% of the page - adjust as desired

\usepackage{multicol}

\bibliography{../refs.bib}

\title{Papers Reviewed for MSc. Thesis}
\author{Christian Baumann}

\begin{document}
\begin{multicols}{2}
	
	\maketitle
	
	\section{Introduction}
	\label{sec:intro}
	
	This document contains a list of papers I consider to
	be interesting or related to my Master's thesis.
	The order of the described papers is quite arbitrary.
	
	\section{Categories}
	
	This section lists papers that are similar in certain aspects.
	
	\subsection{Reinforcement Learning}
	
	The following papers use some sort of reinforcement learning 
	to tackle the control problem: 
	\cite{7401112, 6102330, 8060306, CHEN2018195, 7178338, 6695263, LDPWB2012, 
	8727484, 8335743, 7056534, 4717266}.
	
	\subsection{Convex Optimization Based Control}
	
	E.g. MPC or Linear programming. \cite{4717266}
	
	\subsection{Electric Vehicles}
	
	\subsection{Residental Appliances}
	
	E.g. HVAC, heat pumps, water heaters, natural ventilation.
	
	\section{Summaries}
	
	This sections provides titles and very short summaries of the papers.
	
	\subsection{\citetitle{en11082010}, \cite{en11082010} }
	
	Energy management system for building equipped with PV,
	V2G enabled EV and an energy storage system (ESS). 
	Only controlling whether to buy / sell energy and charge / discharge
	ESS, not controlling EV charging, but no known arrival / departure time
	of EV. Using real-time electricity prices and minimizing total cost.
	Q-learning, $\epsilon$-greedy, with initialization of Q-table as 
	immediate reward to speed up convergence. Simulated in Matlab.
	
	\subsection{\citetitle{7401112}, \cite{7401112} }
	
	Control of either electric water heater or heat pump
	using RL. Extension of fitted Q-iteration that can take into account 
	forecasted data. Also: Policy improvement using expert knowledge
	and 1-day ahead planning using Monte Carlo search method.
	
	\subsection{\citetitle{6596523}, \cite{6596523} }
	
	Electric vehicle and home energy scheduling, minimizing
	electricity costs, 1-day ahead optimization using linear programming
	and a simplified dynamics model
	with known driving patterns and electricity prices.
	Comparing one household with one EV and multiple, 
	bidirectional charging, simulation only.
	
	\subsection{\citetitle{6892986}, \cite{6892986}}
	
	Joint home energy management minimizing electricity costs
	taking into account real-time electricity prices. Stochastic
	programming used to find control policy. Loads include HVAC, EV
	 (unidirectional charging only),
	washing machine, dryer, additionally PVs provide electricity
	and a heat tank is used to store energy. 
	
	\subsection{\citetitle{7552560}, \cite{7552560}}
	
	Joint energy management of data centers and electric vehicles
	of employees, unidirectional charging only, 1-day ahead
	planning with convex optimization. Simulation only.
	
	\subsection{\citetitle{6102330}, \cite{6102330}}
	
	RL used to control EV by either charging, discharging of provide frequency regulation
	to the grid provider. $\epsilon$-greedy Q-learning, trained using separately either MC pricing 
	or past pricing data, no future prices known, control based on only current (and past) price.
	Maximizing profit of the EV owner. 
	
	\subsection{\citetitle{8060306}, \cite{8060306}}
	
	As title suggests, deep RL for HVAC control in building.
	Q-learning, $\epsilon$-greedy, using EnergyPlus simulation tool for training 
	and evaluation. No dynamics model.
	
	\subsection{\citetitle{CHEN2018195}, \cite{CHEN2018195}}
	
	Model-free Q-learning to control HVAC and window systems.
	Learned and evaluated on a simulation and compared to 
	a heuristic approach. Only current data included in state, 
	no future knowledge of temperature, wind or solar irradiation
	included.
	
	\subsection{\citetitle{7178338}, \cite{7178338}}
	
	PEV charging optimization minimizing electricity price. 
	Uses known electricity prices for one day and forecasted
	prices for second day. Fitted Q-iteration with kernel based
	approximation of value iteration. Simulation only. 
	
	\subsection{\citetitle{6695263}, \cite{6695263}}
	
	PEV charging optimization minimizing electricity price. 
	Uses known electricity prices for one day and forecasted
	prices for second day. SARSA with eligibility traces, $\epsilon$-greedy.
	Simulation with real world data. 
	
	\subsection{\citetitle{6547831}, \cite{6547831}}
	
	Residental and EV load, supporting bidirectional charging.
	1-day ahead (distributed) convex optimization minimizing 
	electricity costs, known but varying electricity prices.
	Worst-case-uncertainty approach to tackle uncertainty.
	
	\subsection{\citetitle{HABIB2015205}, \cite{HABIB2015205}}
	
	V2G and charging strategy analysis, high level, no concrete
	algorithms how to coordinate charging, look at references for that.
	
	\subsection{\citetitle{LDPWB2012}, \cite{LDPWB2012}}
	
	Method to tackle the bias induced by taking the max of the 
	estimated Q function in the
	Q-learning algorithm, leading to improved convergence. 
	Tested for battery control with the goal of maximizing 
	profit with varying electricity prices.
	
	\subsection{\citetitle{8727484}, \cite{8727484}}
	
	Joint control of multiple charging stations with the goal
	of load flattening. Probabilistic arrival of new EVs only. 
	Fitted Q-iteration RL algorithm used for optimization, trained based on
	past experience only.
	Simulation using real-world data.
	
	\subsection{\citetitle{4802962}, \cite{4802962}}
	
	Residental PV system and PHEV charging controller is designed
	using fixed load profiles and solar irradiance data. Classical 
	control, simulation only, unidirectional charging. Worst-day
	robustness.
	
	\subsection{\citetitle{8335743}, \cite{8335743}}
	
	Coordinating multiple microgrids with EVs moving between them.
	Bidirectional charging supporting EVs can be used to transport energy
	from one to another microgrid.
	Solved with dynamics model and value iteration \emph{and} model-free
	reinforcement learning (Q-learning).
	
	\subsection{\citetitle{7056534}, \cite{7056534}}
	
	RL for 1-day ahead charging planning of an EV fleet. Fitted Q-iteration,
	Boltzmann exploration, 
	compared to stochastic programming that uses exact EV information.
	RL controlling whole fleet demand, partitioning to individual EVs
	by heuristic.	
	
	\subsection{\citetitle{4717266}, \cite{4717266}}
	
	Comparing MPC to RL (fitted Q-iteration) for a deterministic problem.
	Extra-Tree regressor used as supervised learning algorithm for FQI.
	RL shows comparative performance without knowing the system model.
	RL even shows lower CPU time for online application compared
	to MPC with large time horizons.
	
	\subsection{\citetitle{SMARRA20181252}, \cite{SMARRA20181252}}
	
	Data driven MPC, learning a dynamics model by a regression tree or
	random forest with decision tree partitioning disturbance and state variables
	only. Then in each leave learn the dynamics model, affine linear in control
	variables. Then use this model for receding horizon control as in MPC. 
	Applied to building control.
	
	\printbibliography
	
\end{multicols}
\end{document}

